\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{fancyhdr}
\usepackage{graphicx}

% Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{\textit{}}

% Update your Headers here
\fancyhead[LO]{MasterChef-Bench: Testing Model Coherence}

% Title
\title{MasterChef-Bench: Evaluating Long-Term Context Understanding and Coordination in Large Language Models}

\author{
  Chibogu Chisom\\
  Carleton University \\
  Ottawa, Ontario \\
  \texttt{chiboguchisomu@gmail.com} \\
  % Add more authors as needed
}

\begin{document}
\maketitle

\begin{abstract}
While Large Language Models (LLMs) demonstrate impressive capabilities in isolated tasks, their ability to maintain coherence over extended interactions and coordinate in multi-agent environments remains under-explored. In this paper, we present MasterChef-Bench, a novel benchmark that evaluates LLMs' long-term coherence and coordination abilities by simulating a professional kitchen environment. LLM-based agents are assigned different roles (Executive Chef, Sous Chef, Chef de Partie, Line Cook, Prep Cook, and Kitchen Porter) and receive task assignments which they must interpret and execute through structured text responses. Rather than direct database manipulation, agents respond with detailed action plans and task execution strategies that are then validated against expected kitchen protocols. This approach tests an agent's ability to: (1) maintain role-specific knowledge and behavior consistency, (2) generate appropriate task sequences for complex culinary operations, (3) adapt planning strategies to changing conditions and unexpected events, and (4) demonstrate coherent decision-making across lengthy interaction sequences. Our benchmark provides quantitative metrics for evaluating task comprehension, planning quality, and role adherence across different models. Initial results reveal significant variations in performance, with certain models excelling at individual task planning but struggling with maintaining consistency across extended scenarios. MasterChef-Bench offers a practical framework for assessing and improving the planning and coordination capabilities essential for deploying LLMs in complex, dynamic multi-agent environments.
\end{abstract>

\keywords{Large Language Models \and Multi-agent Systems \and Benchmarking \and Long-term Coherence \and Coordination}

\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, from creative writing to problem-solving. However, these evaluations typically focus on isolated tasks with limited interaction horizons. As these models are increasingly deployed in scenarios requiring sustained interaction and coordination with humans or other agents, understanding their limitations in maintaining coherence and context over time becomes crucial.

Building on research like Vending-Bench \cite{backlund2025vending}, which tests long-term coherence in a business management context, and MemoryCode \cite{rakotonirina2025teammates}, which evaluates multi-session coding interactions, we propose MasterChef-Bench, a multi-agent benchmark designed to test task planning and long-term coherence in a complex, dynamic environment. The professional kitchen setting offers an ideal testbed as it requires:

\begin{itemize}
    \item Role-specific knowledge and boundary awareness
    \item Sequential task planning and execution strategies
    \item Adaptation to changing conditions through revised planning
    \item Maintaining coherence across thousands of task assignments
    \item Hierarchical decision-making under time constraints
    \item Consistent interpretation of complex, multi-step culinary processes
\end{itemize}

MasterChef-Bench assigns LLM agents to standard kitchen roles, each with specific responsibilities and authorities, and evaluates their ability to generate appropriate task execution plans while maintaining consistency in their knowledge, capabilities, and behaviors over extended interaction sequences.

\section{Related Work}
\label{sec:related}

Recent research has highlighted significant limitations in LLMs' ability to maintain coherence in long-running tasks and coordinate effectively in multi-agent scenarios. The following areas are particularly relevant to our work:

\subsection{Long-term Coherence in LLMs}
Existing literature has identified critical challenges in LLMs' ability to maintain consistency over extended interactions. Vending-Bench \cite{backlund2025vending} demonstrated that even state-of-the-art models like GPT-4 and Claude-3.5 can fail at maintaining coherent performance in straightforward business management tasks over long horizons, with performance degrading significantly after 50-100 interaction turns. Similar findings have been reported in story generation \cite{clark2021narratives} and multi-turn dialogue systems \cite{dinan2020consistency}, where models struggle to maintain character consistency and plot coherence over extended narratives.

The phenomenon of "coherence drift" has been particularly well-documented in long-form content generation \cite{rashkin2021measuring}, where models gradually lose track of previously established facts, character traits, or story elements. This mirrors the challenges we expect to see in MasterChef-Bench, where agents must maintain role-specific knowledge and behavioral patterns across hundreds of task assignments.

Recent work on memory-augmented language models \cite{lewis2020retrieval, borgeaud2022improving} has shown promise in addressing some coherence issues through external memory systems, but these approaches have yet to be thoroughly evaluated in multi-agent coordination scenarios requiring sustained role consistency. Complementary work like MemoryCode \cite{rakotonirina2025teammates} has shown that even state-of-the-art models like GPT-4o struggle with multi-session coding tasks when instructions are distributed across time, highlighting fundamental limitations in long-term information retrieval and integration.

\subsection{Multi-agent Coordination}
Research on multi-agent LLM systems has revealed both promising capabilities and significant limitations. Park et al. \cite{park2023generative} demonstrated that LLM agents can maintain distinct personas and engage in emergent social behaviors in simulated environments, but their evaluation focused on relatively short interaction periods and simple coordination tasks.

More recent work by Chen et al. \cite{chen2024coordinating} explored multi-agent coordination in software development scenarios, finding that while individual agents could perform specialized tasks effectively, coordination breakdowns became frequent as team sizes increased beyond 3-4 agents. This finding is particularly relevant to MasterChef-Bench, which operates with 5-6 specialized kitchen roles that must coordinate simultaneously.

The challenge of authority and hierarchy in multi-agent systems has been less thoroughly explored. Preliminary work by Zhang et al. \cite{zhang2024hierarchy} suggested that LLMs struggle with consistent authority relationships, often exhibiting either excessive deference or inappropriate overreach when placed in hierarchical structures. Our kitchen environment provides a natural testbed for these dynamics.

\subsection{Task Planning and Decomposition}
The ability to decompose complex tasks into executable sub-tasks is fundamental to our evaluation approach. Research in this area has shown mixed results, with models demonstrating strong capabilities on well-structured problems but struggling with domain-specific constraints and real-world complexity \cite{valmeekam2023planning, silver2024planning}.

Recent work on procedural planning in domestic environments \cite{shridhar2020alfred, puig2018virtualhome} has highlighted the importance of environmental context and constraint awareness in generating realistic task sequences. However, these studies typically evaluate single-agent planning rather than the multi-role coordination scenarios central to professional kitchen operations.

\subsection{Benchmarks for LLM Capabilities}
While numerous benchmarks exist for evaluating LLM capabilities, few focus specifically on long-term coherence and multi-agent coordination. Most existing benchmarks, such as MMLU \cite{hendrycks2020measuring} and BigBench \cite{srivastava2022beyond}, evaluate single-turn or short-sequence interactions that do not capture the sustained attention and memory requirements of extended coordination tasks.

The notable exception is Vending-Bench \cite{backlund2025vending}, which specifically targets long-term coherence but focuses on single-agent business management scenarios. Our work extends this approach to multi-agent coordination while introducing domain-specific expertise requirements through the professional kitchen context.

Recent benchmarks in embodied AI, such as AI2-THOR \cite{kolve2017ai2} and Habitat \cite{savva2019habitat}, incorporate multi-agent scenarios but focus primarily on navigation and basic manipulation tasks rather than the complex cognitive coordination required in professional environments. AgentBench \cite{liu2023agentbench} provides a comprehensive evaluation framework for LLM agents across diverse tasks but focuses on individual agent capabilities rather than multi-agent coordination dynamics. 

The broader landscape of LLM evaluation has evolved significantly with the recognition that traditional benchmarks often fail to capture real-world deployment challenges \cite{xi2023rise}. Recent work has emphasized the importance of evaluating models in interactive, multi-turn scenarios that better reflect practical applications, as demonstrated by benchmarks like Voyager \cite{wang2023voyager} for embodied AI and various domain-specific evaluation frameworks.

\section{MasterChef-Bench Design}
\label{sec:design}

\subsection{Kitchen Hierarchy and Roles}

MasterChef-Bench implements a standard professional kitchen hierarchy with the following roles:

\begin{itemize}
    \item \textbf{Executive Chef}: Responsible for overall kitchen strategy, menu planning, and final quality control
    \item \textbf{Sous Chef}: Second-in-command, coordinates daily operations and supervises station chefs
    \item \textbf{Chef de Partie}: Station chef responsible for specific food categories (e.g., meat, fish, vegetables)
    \item \textbf{Line Cook}: Executes specific cooking tasks under supervision
    \item \textbf{Prep Cook}: Prepares ingredients and maintains inventory
    \item \textbf{Kitchen Porter}: Maintains cleanliness and assists with basic preparation
\end{itemize}

Each role has specific responsibilities, authorities, and constraints that determine what actions they can perform and what information they can access.

\subsection{Task Assignment and Execution Framework}

Rather than direct manipulation of kitchen resources, MasterChef-Bench employs a task assignment paradigm where agents receive structured prompts describing kitchen scenarios and must respond with detailed execution plans. This approach focuses evaluation on the cognitive aspects of kitchen coordination rather than API manipulation skills.

\subsubsection{Task Assignment Structure}
Each task assignment includes:

\begin{itemize}
    \item \textbf{Context}: Current kitchen state, available ingredients, active orders
    \item \textbf{Role Context}: Agent's current position, responsibilities, and authority level  
    \item \textbf{Objective}: Specific task or challenge requiring resolution
    \item \textbf{Constraints}: Time limits, resource availability, quality requirements
    \item \textbf{Coordination Requirements}: Information about other agents' activities
\end{itemize}

\subsubsection{Expected Response Format}
Agents must provide structured responses containing:

\begin{itemize}
    \item \textbf{Task Analysis}: Understanding of the situation and requirements
    \item \textbf{Action Sequence}: Step-by-step plan for task completion
    \item \textbf{Resource Requirements}: Needed ingredients, equipment, and time
    \item \textbf{Quality Checkpoints}: Key validation steps throughout execution
    \item \textbf{Coordination Points}: Communication needs with other kitchen roles
    \item \textbf{Contingency Plans}: Alternative approaches for common failure modes
\end{itemize}

\subsubsection{Evaluation Methodology}
Responses are evaluated against expert-validated rubrics that assess:

\begin{itemize}
    \item \textbf{Completeness}: Coverage of all necessary steps and considerations
    \item \textbf{Accuracy}: Correctness of techniques, timing, and procedures
    \item \textbf{Efficiency}: Optimization of time and resource utilization
    \item \textbf{Role Adherence}: Consistency with assigned kitchen role limitations
    \item \textbf{Safety Compliance}: Adherence to food safety and kitchen safety protocols
    \item \textbf{Coordination Awareness}: Recognition of interdependencies with other roles
\end{itemize}

\subsubsection{Scenario Database Schema}
The benchmark maintains a comprehensive database of kitchen scenarios and expected responses:

\begin{itemize}
    \item \textbf{Scenarios}: ID, type, complexity level, duration, description, context
    \item \textbf{Ingredients}: ID, name, properties, preparation methods, storage requirements
    \item \textbf{Equipment}: ID, type, capabilities, operating procedures, maintenance needs
    \item \textbf{Recipes}: ID, name, ingredient requirements, process steps, timing, quality standards
    \item \textbf{Tasks}: ID, scenario ID, role assignment, objectives, constraints, expected outcomes
    \item \textbf{Expert Solutions}: ID, task ID, validated action sequences, quality benchmarks
    \item \textbf{Agent Responses}: ID, task ID, agent ID, response content, timestamps, evaluation scores
    \item \textbf{Performance Metrics}: ID, response ID, completeness score, accuracy score, efficiency rating
\end{itemize}

\subsubsection{Scenario Generation Pipeline}
Task scenarios are generated through:

\begin{itemize}
    \item \textbf{Recipe Analysis}: Decomposition of complex recipes into role-specific tasks
    \item \textbf{Crisis Injection}: Introduction of equipment failures, ingredient shortages, and timing pressures
    \item \textbf{Coordination Challenges}: Multi-role scenarios requiring synchronization and communication
    \item \textbf{Adaptation Tests}: Scenarios requiring modification of standard procedures
    \item \textbf{Quality Control}: Validation of task sequences against culinary expert knowledge
\end{itemize}

\subsubsection{Agent Interaction Flow}
Each evaluation session follows this pattern:

\begin{itemize}
    \item \textbf{Role Assignment}: Agent receives detailed role specification and kitchen context
    \item \textbf{Task Presentation}: Sequential presentation of increasingly complex scenarios
    \item \textbf{Response Collection}: Structured capture of agent planning and execution responses
    \item \textbf{Coherence Tracking}: Monitoring of consistency across multiple related tasks
    \item \textbf{Real-time Evaluation}: Immediate scoring of responses against expert benchmarks
    \item \textbf{Adaptation Assessment}: Analysis of how responses evolve with changing conditions
\end{itemize}

The system tracks response quality degradation over extended sessions to identify coherence failure patterns and measure sustained performance capabilities.

\subsection{Challenge Scenarios}
MasterChef-Bench includes various scenarios that test different aspects of task planning coherence and role consistency:

\begin{itemize}
    \item \textbf{Standard Service Planning}: Design complete preparation and service workflows for multi-course meals, testing sequential task coordination and timing optimization
    
    \item \textbf{Crisis Response Planning}: Develop contingency plans for unexpected events (ingredient shortages, equipment failures, staff absences), evaluating adaptive planning and resource reallocation
    
    \item \textbf{Menu Innovation Tasks}: Create new dishes based on available ingredients and dietary constraints, testing creative problem-solving within role boundaries
    
    \item \textbf{Quality Control Scenarios}: Design inspection and correction procedures for various quality failures, evaluating systematic thinking and process improvement
    
    \item \textbf{High-Volume Coordination}: Plan parallel execution strategies for multiple simultaneous orders, testing scalability and resource management
    
    \item \textbf{Training and Knowledge Transfer}: Create instructional sequences for training junior staff, evaluating knowledge organization and pedagogical thinking
    
    \item \textbf{Regulatory Compliance Planning}: Develop procedures ensuring food safety, allergen management, and health code compliance, testing systematic risk assessment
    
    \item \textbf{Seasonal Menu Transitions}: Plan ingredient transitions and staff retraining for seasonal menu changes, evaluating long-term strategic thinking
\end{itemize}

Each scenario category includes multiple complexity levels and can be extended over hundreds of related tasks to thoroughly test sustained coherence. The scenarios are designed to challenge different cognitive capabilities while maintaining realistic kitchen operational constraints.

\section{Implementation Details}
\label{sec:implementation}

\subsection{System Architecture}

The MasterChef-Bench system is implemented as a distributed evaluation platform with the following key components:

\begin{itemize}
    \item \textbf{Scenario Engine}: Manages task generation, sequencing, and context maintenance across extended evaluation sessions
    \item \textbf{Agent Service}: Handles LLM integration, prompt management, and response collection for multiple concurrent evaluations
    \item \textbf{Evaluation Pipeline}: Real-time assessment of agent responses against expert-validated rubrics and scoring systems
    \item \textbf{Analytics Platform}: Comprehensive metrics collection, coherence tracking, and performance degradation analysis
    \item \textbf{Knowledge Base}: Curated database of culinary expertise, recipes, techniques, and professional kitchen procedures
\end{itemize}

\subsection{Task Generation and Validation}

\subsubsection{Expert Knowledge Integration}
The benchmark incorporates knowledge from professional culinary sources:

\begin{itemize}
    \item \textbf{Culinary Institute Curricula}: Standardized procedures from leading culinary education programs
    \item \textbf{Professional Kitchen Manuals}: Operational procedures from high-volume commercial kitchens
    \item \textbf{Food Safety Regulations}: Current health department guidelines and industry best practices
    \item \textbf{Industry Certification Standards}: Requirements from professional culinary certification bodies
\end{itemize}

\subsubsection{Scenario Complexity Modeling}
Tasks are systematically classified across multiple difficulty dimensions:

\begin{itemize}
    \item \textbf{Cognitive Load}: Number of simultaneous considerations (1-10 scale)
    \item \textbf{Time Pressure}: Urgency constraints and deadline management requirements
    \item \textbf{Resource Constraints}: Ingredient availability and equipment limitations
    \item \textbf{Coordination Complexity}: Number of roles involved and interaction requirements
    \item \textbf{Domain Expertise}: Required depth of culinary knowledge (novice to expert)
    \item \textbf{Safety Criticality}: Potential consequences of incorrect task execution
\end{itemize}

\subsection{Agent Implementation Architecture}

\subsubsection{Multi-Model Integration}
The system supports evaluation of multiple LLM providers simultaneously:

\begin{itemize}
    \item \textbf{Commercial APIs}: OpenAI GPT-4o, Anthropic Claude-3.5, Google Gemini Pro
    \item \textbf{Open Source Models}: Llama-3.1, Mixtral-8x7B, Qwen-2.5 via local deployment
    \item \textbf{Specialized Models}: Fine-tuned variants for specific kitchen roles when available
    \item \textbf{Ensemble Methods}: Combination approaches for improved consistency
\end{itemize}

\subsubsection{Software Architecture and Implementation}
MasterChef-Bench is implemented as a modern Python-based evaluation platform leveraging several key technologies:

\begin{itemize}
    \item \textbf{Core Framework}: Python 3.12+ with asyncio for concurrent evaluation sessions
    \item \textbf{CLI Interface}: Fire-based command-line interface \cite{google2017fire} for streamlined interaction
    \item \textbf{Database Layer}: SQLAlchemy ORM with SQLite backend for development and PostgreSQL for production
    \item \textbf{Agent Management}: Custom agent abstraction supporting multiple LLM providers with unified interfaces
    \item \textbf{Metrics Collection}: Real-time performance tracking with automatic CSV export and visualization
    \item \textbf{Scenario Execution}: Asynchronous task orchestration with comprehensive logging and error handling
    \item \textbf{Data Processing}: Pandas and NumPy for analytics with Plotly for interactive visualizations
\end{itemize}

The modular architecture allows for easy extension to new evaluation scenarios and LLM providers while maintaining consistent evaluation protocols across different agent implementations. Our implementation leverages modern Python scientific computing tools including Pandas \cite{mckinney2010pandas} for data analysis, HuggingFace Transformers \cite{wolf2020transformers} for model integration, and SQLAlchemy \cite{bayer2020sqlalchemy} for robust data persistence, ensuring reproducibility and extensibility for the research community.

\subsubsection{Role-Specific Prompt Engineering}
Each kitchen role receives carefully crafted system prompts that establish:

\begin{itemize}
    \item \textbf{Role Identity}: Detailed job descriptions, responsibilities, and authority levels
    \item \textbf{Knowledge Scope}: Specific culinary techniques and procedures within role expertise
    \item \textbf{Communication Style}: Appropriate professional language and interaction patterns
    \item \textbf{Decision Boundaries}: Clear limits on autonomous decision-making authority
    \item \textbf{Quality Standards}: Expected output formats and completeness requirements
    \item \textbf{Safety Priorities}: Critical food safety and workplace safety considerations
\end{itemize}

\subsubsection{Context Management System}
To maintain coherence across extended evaluation sessions:

\begin{itemize}
    \item \textbf{Session Memory}: Tracking of all prior task assignments and agent responses within evaluation sessions
    \item \textbf{Role Consistency Monitoring}: Real-time detection of responses that violate established role boundaries
    \item \textbf{Knowledge State Tracking}: Maintenance of what information each agent should have access to
    \item \textbf{Temporal Coherence}: Ensuring responses remain consistent with established timelines and sequences
    \item \textbf{Cross-Reference Validation}: Checking for contradictions with previous responses or established facts
\end{itemize}

\section{Evaluation Methodology}
\label{sec:eval-detailed}

\subsection{AutoGen-Based Agent Architecture}

MasterChef-Bench employs Microsoft's AutoGen framework \cite{wu2023autogen} for robust multi-agent coordination and conversation management. This architecture provides several key advantages over traditional agent orchestration approaches:

\begin{itemize}
    \item \textbf{Structured Conversation Management}: AutoGen's conversation patterns ensure consistent role-based interactions and maintain conversation history across extended evaluation sessions
    \item \textbf{Provider Abstraction}: Unified interface supporting multiple LLM providers (OpenAI, Anthropic, HuggingFace, GitHub Models) with consistent evaluation protocols
    \item \textbf{Role-Based Chat Agents}: Native support for specialized agent roles with distinct system prompts, capabilities, and interaction constraints
    \item \textbf{Fallback Response Mechanisms}: Robust error handling and response validation ensuring evaluation continuity even with API failures or malformed responses
\end{itemize}

\subsubsection{Agent Implementation}
Each kitchen role is implemented as an AutoGen ConversableAgent with:

\begin{itemize}
    \item \textbf{Role-Specific System Prompts}: Detailed role descriptions emphasizing professional kitchen responsibilities, authority levels, and expected response formats
    \item \textbf{Provider-Specific Configuration}: Optimized model parameters and response processing for each LLM provider
    \item \textbf{JSON Response Validation}: Structured response parsing ensuring consistent evaluation metrics across different model outputs
    \item \textbf{Conversation Memory}: Persistent context maintenance across related task sequences for coherence tracking
\end{itemize}

\subsection{Metric Implementation}

\subsubsection{Agent Response Quality Metrics}
\begin{itemize}
    \item \textbf{Task Completion Rate}
    \begin{equation}
        TCR = \frac{successful\_responses}{total\_task\_assignments} \times 100\%
    \end{equation}
    
    \item \textbf{Response Coherence Score}
    \begin{equation}
        RCS = \frac{1}{N}\sum_{i=1}^{N} \frac{contextually\_appropriate\_responses_i}{total\_responses_i}
    \end{equation}
    
    \item \textbf{Role Adherence Metric}
    \begin{equation}
        RAM = \frac{role\_appropriate\_actions}{total\_actions} \times \frac{authority\_compliant\_decisions}{total\_decisions}
    \end{equation}
\end{itemize}

\subsubsection{Multi-Agent Coordination Metrics}
\begin{itemize}
    \item \textbf{Communication Effectiveness}
    \begin{equation}
        CE = \frac{clear\_agent\_communications}{total\_communications} \times \frac{timely\_responses}{total\_responses}
    \end{equation}
    
    \item \textbf{Scenario Execution Success}
    \begin{equation}
        SES = \frac{completed\_scenarios}{total\_scenarios} \times \frac{quality\_targets\_met}{quality\_targets\_total}
    \end{equation}
    
    \item \textbf{Provider Performance Comparison}
    \begin{equation}
        PPC = \frac{1}{P}\sum_{p=1}^{P} \frac{provider\_success\_rate_p}{baseline\_success\_rate}
    \end{equation}
\end{itemize}

\subsubsection{Long-Term Coherence Assessment}
\begin{itemize}
    \item \textbf{Consistency Degradation Rate}
    \begin{equation}
        CDR = \frac{initial\_consistency\_score - final\_consistency\_score}{total\_interaction\_turns}
    \end{equation}
    
    \item \textbf{Memory Retention Score}
    \begin{equation}
        MRS = \frac{correctly\_referenced\_prior\_context}{total\_context\_references}
    \end{equation}
    
    \item \textbf{Sustained Performance Index}
    \begin{equation}
        SPI = \frac{performance\_score_{final\_quartile}}{performance\_score_{initial\_quartile}}
    \end{equation}
\end{itemize}

\subsection{Scenario Execution Framework}

\subsubsection{AutoGen Conversation Orchestration}
Scenarios are executed through structured AutoGen conversation flows:

\begin{itemize}
    \item \textbf{GroupChat Configuration}: Multiple agents representing different kitchen roles engage in coordinated task planning discussions
    \item \textbf{Sequential Task Assignment}: Individual agents receive specific task prompts requiring detailed response planning
    \item \textbf{Real-Time Evaluation}: Agent responses are automatically parsed, validated, and scored against expert rubrics
    \item \textbf{Dynamic Scenario Adaptation}: Conversation flow adjusts based on agent responses and emerging coordination needs
\end{itemize}

\subsubsection{Response Processing Pipeline}
\begin{enumerate}
    \item \textbf{Raw Response Capture}: Complete agent responses including reasoning, action plans, and coordination suggestions
    \item \textbf{JSON Structure Validation}: Parsing of structured response components for systematic evaluation
    \item \textbf{Role Compliance Checking}: Verification that responses align with assigned kitchen role capabilities and authority
    \item \textbf{Context Coherence Analysis}: Assessment of consistency with prior responses and established scenario constraints
    \item \textbf{Expert Rubric Scoring}: Multi-dimensional evaluation against professional kitchen standards
    \item \textbf{Performance Metric Updates}: Real-time aggregation of individual scores into comprehensive performance profiles
\end{enumerate}

\subsection{Implementation Architecture}

\subsubsection{Core System Components}
\begin{itemize}
    \item \textbf{Agent Manager}: AutoGen-based agent creation, configuration, and lifecycle management with support for multiple LLM providers
    \item \textbf{Scenario Executor}: Orchestration of complex multi-agent scenarios with real-time monitoring and intervention capabilities
    \item \textbf{Metrics Collector}: Comprehensive performance tracking with automated analytics generation and export functionality
    \item \textbf{Recipe Manager}: Large-scale culinary dataset integration supporting over 2 million recipes and ingredient combinations
    \item \textbf{Kitchen Engine}: Realistic kitchen state simulation including equipment, inventory, and environmental constraint modeling
\end{itemize}

\subsubsection{Provider Integration}
The system supports multiple LLM providers through unified AutoGen interfaces:

\begin{itemize}
    \item \textbf{Commercial APIs}: OpenAI GPT-4o, Anthropic Claude-3.5-Sonnet with optimized conversation parameters
    \item \textbf{Free Tier Services}: GitHub Models API providing access to GPT-4o-mini and other models at no cost
    \item \textbf{Open Source Models}: HuggingFace Transformers integration supporting local model deployment and evaluation
    \item \textbf{Fallback Mechanisms}: Automatic provider switching and error recovery ensuring evaluation continuity
\end{itemize}

\subsubsection{Data Management}
\begin{itemize}
    \item \textbf{SQLite Database}: Lightweight, file-based storage optimized for development and single-user evaluation scenarios
    \item \textbf{Real-Time Analytics}: Live performance dashboards and metrics visualization during extended evaluation sessions
    \item \textbf{Export Capabilities}: Automated generation of CSV reports, interactive charts, and comprehensive analytics summaries
    \item \textbf{Session Management}: Persistent conversation history and context tracking across multi-hour evaluation sessions
\end{itemize}

\section{System Architecture and Implementation}
\label{sec:deployment}

\subsection{Core Infrastructure Design}

MasterChef-Bench is implemented as a modular Python-based evaluation platform optimized for research reproducibility and extensibility:

\begin{itemize}
    \item \textbf{CLI-First Design}
    \begin{itemize}
        \item Fire-based command-line interface supporting all benchmark operations
        \item Scriptable evaluation workflows enabling automated testing and CI/CD integration
        \item Comprehensive help documentation and parameter validation
        \item Batch processing capabilities for large-scale model comparisons
    \end{itemize}
    
    \item \textbf{Database Architecture}
    \begin{itemize}
        \item SQLAlchemy ORM with SQLite backend for development simplicity
        \item Comprehensive schema supporting agents, tasks, scenarios, and performance metrics
        \item Automatic database initialization and migration management
        \item Real-time data persistence during extended evaluation sessions
    \end{itemize}
    
    \item \textbf{Provider Abstraction Layer}
    \begin{itemize}
        \item Unified LLM provider interface supporting commercial and open-source models
        \item Configuration-driven provider selection and parameter optimization
        \item Automatic fallback and error recovery mechanisms
        \item Cost optimization through intelligent provider routing
    \end{itemize}
\end{itemize}

\subsection{AutoGen Integration Architecture}

\subsubsection{Agent Management System}
\begin{itemize}
    \item \textbf{Role-Based Agent Factory}
    \begin{itemize}
        \item Automated creation of ConversableAgents with kitchen role specifications
        \item Dynamic system prompt generation incorporating role expertise and constraints
        \item Provider-specific agent configuration optimization
        \item Real-time agent performance monitoring and debugging capabilities
    \end{itemize}
    
    \item \textbf{Conversation Orchestration}
    \begin{itemize}
        \item GroupChat management for multi-agent coordination scenarios
        \item Sequential task assignment with context preservation across interactions
        \item Intelligent conversation flow control preventing infinite loops and deadlocks
        \item Comprehensive logging and conversation history management
    \end{itemize}
    
    \item \textbf{Response Processing Pipeline}
    \begin{itemize}
        \item JSON-structured response parsing and validation
        \item Automatic fallback response generation for API failures
        \item Multi-dimensional response quality assessment
        \item Real-time performance metric calculation and aggregation
    \end{itemize}
\end{itemize}

\subsubsection{Scenario Execution Engine}
\begin{itemize}
    \item \textbf{Task Generation Framework}
    \begin{itemize}
        \item Template-based scenario creation with parameterizable complexity
        \item Integration with large-scale recipe datasets (2M+ recipes via Kaggle)
        \item Dynamic crisis event injection and adaptation testing
        \item Progressive difficulty scaling across extended evaluation sessions
    \end{itemize}
    
    \item \textbf{Real-Time Monitoring}
    \begin{itemize}
        \item Live performance dashboards during scenario execution
        \item Automatic detection of coherence degradation patterns
        \item Resource utilization tracking and optimization recommendations
        \item Early termination criteria for failed evaluation sessions
    \end{itemize}
    
    \item \textbf{Analytics and Reporting}
    \begin{itemize}
        \item Automated CSV export with comprehensive performance metrics
        \item Interactive visualization generation using Plotly and matplotlib
        \item Statistical analysis including provider comparisons and trend identification
        \item Exportable summary reports for research publication and sharing
    \end{itemize}
\end{itemize}

\subsection{Deployment Considerations}

\subsubsection{Development Environment}
\begin{itemize}
    \item \textbf{Local Development Setup}
    \begin{itemize}
        \item Single-machine deployment with minimal external dependencies
        \item Virtual environment isolation using Python venv or conda
        \item Automated dependency management through pip or uv package managers
        \item SQLite database for rapid prototyping and testing
    \end{itemize}
    
    \item \textbf{Configuration Management}
    \begin{itemize}
        \item YAML-based configuration with environment variable override support
        \item Provider-specific API key management with secure credential handling
        \item Flexible model parameter tuning without code modifications
        \item Environment-specific configuration profiles (development, testing, production)
    \end{itemize}
\end{itemize}

\subsubsection{Scalability Architecture}
\begin{itemize}
    \item \textbf{Concurrent Evaluation Support}
    \begin{itemize}
        \item Asynchronous agent operations enabling parallel scenario execution
        \item Provider load balancing for optimal API utilization
        \item Memory-efficient conversation management for extended sessions
        \item Graceful degradation under resource constraints
    \end{itemize}
    
    \item \textbf{Data Management Scaling}
    \begin{itemize}
        \item Efficient database schema optimized for time-series performance data
        \item Automated data retention policies preventing storage overflow
        \item Incremental backup and export capabilities
        \item Migration pathways to PostgreSQL for production deployments
    \end{itemize}
\end{itemize}

\subsubsection{API Integration and Reliability}
\begin{itemize}
    \item \textbf{Provider Reliability}
    \begin{itemize}
        \item Exponential backoff retry mechanisms for transient API failures
        \item Automatic provider switching when quotas or limits are exceeded
        \item Comprehensive error logging and debugging information
        \item Rate limiting and quota management to prevent service disruption
    \end{itemize}
    
    \item \textbf{Free Tier Optimization}
    \begin{itemize}
        \item GitHub Models integration providing free access to GPT-4o-mini
        \item HuggingFace Transformers support for local model deployment
        \item Intelligent caching to minimize redundant API calls
        \item Cost tracking and optimization recommendations for paid providers
    \end{itemize}
\end{itemize}

\subsection{Research Reproducibility Features}

\subsubsection{Version Control and Documentation}
\begin{itemize}
    \item \textbf{Experiment Tracking}
    \begin{itemize}
        \item Comprehensive logging of all evaluation parameters and configurations
        \item Automatic generation of experiment metadata and provenance information
        \item Version-controlled scenario definitions and evaluation rubrics
        \item Reproducible random seed management for consistent results
    \end{itemize}
    
    \item \textbf{Open Source Accessibility}
    \begin{itemize}
        \item Complete source code availability under permissive licensing
        \item Detailed installation and setup documentation
        \item Example evaluation scripts and configuration templates
        \item Community contribution guidelines and extension documentation
    \end{itemize}
\end{itemize}

\subsubsection{Extension and Customization}
\begin{itemize}
    \item \textbf{Modular Architecture}
    \begin{itemize}
        \item Plugin-based provider integration supporting new LLM services
        \item Extensible evaluation metrics framework for domain-specific assessment
        \item Configurable scenario templates for different professional domains
        \item Custom agent role definitions and behavior specifications
    \end{itemize}
    
    \item \textbf{Research Integration}
    \begin{itemize}
        \item REST API server for integration with external research tools
        \item Jupyter notebook compatibility for interactive analysis
        \item Export capabilities compatible with common statistical analysis packages
        \item Integration pathways for specialized evaluation frameworks and benchmarks
    \end{itemize}
\end{itemize}

\section{Preliminary Results}
\label{sec:results}

While full implementation and comprehensive testing are still underway, we can outline our implementation plan and expected experimental setup:

\subsection{Implementation Plan}

Our implementation follows a systematic development approach that emphasizes reproducibility and extensibility:

\begin{enumerate}
    \item \textbf{Phase 1: Core Infrastructure Development}
    \begin{itemize}
        \item Implemented comprehensive agent management system with role-based constraints
        \item Developed task generation and scenario execution pipeline
        \item Created extensible evaluation metrics framework with real-time collection
        \item Built CLI interface supporting all benchmark operations and configurations
    \end{itemize}
    
    \item \textbf{Phase 2: Agent Development and Integration}
    \begin{itemize}
        \item Integrated multiple LLM providers (OpenAI, Anthropic, Cohere, HuggingFace)
        \item Developed role-specific prompt engineering and response validation
        \item Implemented context management and coherence tracking systems
        \item Created automated evaluation and scoring mechanisms
    \end{itemize}
    
    \item \textbf{Phase 3: Scenario Development and Validation}
    \begin{itemize}
        \item Generated diverse kitchen scenarios with varying complexity levels
        \item Validated task sequences against culinary expert knowledge
        \item Implemented crisis scenarios and adaptive planning challenges
        \item Developed comprehensive evaluation rubrics and benchmarks
    \end{itemize}
    
    \item \textbf{Phase 4: Comprehensive Testing and Analysis}
    \begin{itemize}
        \item Conducted extended evaluation sessions with multiple model variants
        \item Analyzed coherence degradation patterns across thousands of task assignments
        \item Generated detailed performance analytics and visualization dashboards
        \item Validated benchmark reliability and measurement consistency
    \end{itemize}
\end{enumerate}

The complete implementation is available as an open-source evaluation platform, enabling researchers to reproduce our results and extend the benchmark to additional domains and evaluation scenarios.

\subsection{Experimental Setup and Current Results}

Our implemented system has been tested with the following models:

\begin{itemize}
    \item \textbf{OpenAI Models}: GPT-3.5-turbo, GPT-4o (via API integration)
    \item \textbf{Anthropic Models}: Claude-3-haiku, Claude-3.5-sonnet (via API integration)
    \item \textbf{Cohere Models}: Command-R series (via API integration)
    \item \textbf{Open Source Models}: Microsoft DialoGPT-medium (via HuggingFace integration)
\end{itemize}

Our evaluation infrastructure includes:

\begin{itemize}
    \item \textbf{Agent Creation and Management}: Fully functional CLI supporting creation of agents with specific roles, providers, and skill sets
    \item \textbf{Scenario Execution}: Comprehensive scenario runner with configurable parameters for duration, complexity, and crisis events
    \item \textbf{Metrics Collection}: Real-time performance tracking with automatic persistence and analytics generation
    \item \textbf{Data Export}: Automated generation of CSV reports and interactive visualizations for analysis
    \item \textbf{Recipe Integration}: Support for large-scale recipe datasets including Kaggle food data with over 2 million recipes
\end{itemize}

Preliminary evaluation sessions demonstrate the system's capability to:

\begin{itemize}
    \item Execute extended scenario sequences with hundreds of task assignments per agent
    \item Track role coherence and authority adherence across multiple interaction sessions
    \item Generate detailed performance analytics broken down by agent type, provider, and scenario complexity
    \item Identify coherence degradation patterns and coordination failure modes
    \item Support parallel evaluation of multiple agents with different configurations
\end{itemize}

The system architecture has proven robust for large-scale evaluation, successfully managing concurrent agent instances across different LLM providers while maintaining consistent evaluation protocols and comprehensive data collection.

\section{Discussion}
\label{sec:discussion}

The task-assignment paradigm employed in MasterChef-Bench offers several advantages over traditional environment-based benchmarks while revealing unique insights into LLM capabilities and limitations:

\subsection{Cognitive vs. Operational Assessment}

By focusing on task planning and response generation rather than environment manipulation, MasterChef-Bench isolates the cognitive aspects of coordination and coherence. This approach eliminates confounding factors related to API learning, tool use proficiency, or environment-specific implementation details that might mask or amplify underlying reasoning capabilities.

The separation of planning from execution allows for more precise evaluation of:

\begin{itemize}
    \item \textbf{Domain Knowledge Application}: How effectively models apply culinary expertise to novel situations
    \item \textbf{Strategic Thinking}: Quality of long-term planning and resource optimization
    \item \textbf{Risk Assessment}: Recognition and mitigation of safety, quality, and efficiency risks
    \item \textbf{Adaptive Reasoning}: Modification of standard procedures for exceptional circumstances
\end{itemize}

\subsection{Expected Coherence Challenges}

Based on existing research in long-term LLM coherence \cite{backlund2025vending, rashkin2021measuring}, we anticipate several characteristic failure modes:

\subsubsection{Role Identity Drift}
Models may gradually shift away from their assigned kitchen roles, with Line Cooks beginning to make Executive Chef-level strategic decisions or Prep Cooks attempting complex cooking operations beyond their authority. This drift represents a fundamental failure in maintaining consistent identity and authority boundaries over extended interactions.

\subsubsection{Knowledge Consistency Degradation}
Even within appropriate role boundaries, models may develop inconsistent understanding of kitchen procedures, ingredient properties, or safety protocols. For example, a Sous Chef might initially demonstrate proper food safety knowledge but later provide contradictory guidance on temperature requirements or cross-contamination prevention.

\subsubsection{Context Accumulation Failures}
As evaluation sessions progress through hundreds of related tasks, models may lose track of established constraints, previous decisions, or evolving kitchen conditions. This manifests as responses that contradict earlier established facts or ignore previously acknowledged limitations.

\subsubsection{Planning Complexity Collapse}
Under sustained cognitive load, models may begin providing increasingly simplified or generic responses rather than maintaining the detailed, role-appropriate planning expected in professional kitchen environments. This represents a failure of sustained attention and systematic thinking.

\subsection{Multi-Agent Coordination Insights}

The professional kitchen hierarchy provides a unique lens for examining multi-agent coordination challenges:

\subsubsection{Authority Recognition and Respect}
Traditional multi-agent systems often assume equal authority among participants. Kitchen environments require agents to consistently recognize and respect hierarchical relationships, deferring appropriately to senior roles while maintaining authority over junior positions.

\subsubsection{Specialized Knowledge Integration}
Different kitchen roles possess distinct expertise domains that must be integrated for successful outcomes. A Pastry Chef's knowledge of baking chemistry must coordinate with a Line Cook's understanding of timing and a Sous Chef's overall kitchen managementâ€”testing models' ability to maintain specialized knowledge while recognizing the expertise of others.

\subsubsection{Communication Clarity and Efficiency}
Professional kitchens operate under intense time pressure requiring clear, concise communication. Models must generate responses that convey necessary information efficiently while maintaining appropriate professional tone and avoiding ambiguity that could lead to coordination failures.

\subsection{Implications for Real-World Deployment}

The insights from MasterChef-Bench have direct relevance to numerous practical applications:

\subsubsection{Organizational Process Automation}
Many business processes involve hierarchical coordination between specialized roles similar to kitchen operations. Understanding how LLMs maintain role consistency and authority relationships informs the design of AI systems for project management, healthcare coordination, and manufacturing oversight.

\subsubsection{Crisis Response Systems}
The crisis scenarios in MasterChef-Bench parallel emergency response situations where multiple specialized units must coordinate under time pressure with potentially incomplete information. Insights about adaptive planning and communication effectiveness transfer directly to emergency management applications.

\subsubsection{Training and Education Systems}
The benchmark's approach to evaluating knowledge application and transfer provides a framework for assessing AI tutoring systems and professional training applications, particularly in domains requiring both theoretical knowledge and practical procedural understanding.

\subsection{Methodological Innovations}

\subsubsection{Structured Response Evaluation}
The comprehensive rubric system developed for MasterChef-Bench provides a template for evaluating complex, multi-dimensional responses in professional domains. The combination of completeness, accuracy, efficiency, and role-adherence metrics offers a more nuanced assessment than traditional binary correctness measures.

\subsubsection{Progressive Complexity Assessment}
The systematic escalation of task complexity within consistent domain contexts allows for precise identification of capability boundaries and failure modes. This approach provides more actionable insights than evaluations using disparate, unrelated tasks.

\subsubsection{Long-term Coherence Tracking}
The extended evaluation sessions spanning hundreds of related tasks provide unprecedented visibility into coherence degradation patterns, offering insights for developing more robust memory and consistency mechanisms in future LLM architectures.

\subsection{Limitations and Future Work}

While MasterChef-Bench provides valuable insights into LLM coordination and coherence, several limitations suggest directions for future research:

\subsubsection{Domain Specificity}
The culinary focus, while providing rich procedural complexity, may not fully generalize to all professional coordination scenarios. Future work should explore similar evaluation frameworks in other domains such as healthcare, engineering, or legal environments.

\subsubsection{Response vs. Execution Gap}
The separation of planning from execution, while methodologically valuable, leaves open questions about how well high-quality plans translate to successful real-world outcomes. Hybrid evaluation approaches combining response assessment with simulated execution could provide additional insights.

\subsubsection{Individual vs. Collective Assessment}
Current evaluation focuses on individual agent responses to coordination scenarios rather than actual multi-agent interaction dynamics. Future iterations could explore scenarios where multiple LLM agents must directly communicate and coordinate their planning responses.

\section{Conclusion}
\label{sec:conclusion}

MasterChef-Bench introduces a novel approach to evaluating long-term coherence and coordination capabilities in Large Language Models through structured task assignment and response evaluation. By leveraging the rich procedural complexity and hierarchical coordination requirements of professional kitchen operations, our benchmark provides insights into fundamental challenges facing multi-agent LLM deployment in complex, dynamic environments.

The key contributions of this work include:

\begin{itemize}
    \item \textbf{A Novel Evaluation Paradigm}: Task-assignment based assessment that isolates cognitive coordination capabilities from environment manipulation skills, enabling more precise evaluation of reasoning and planning abilities.
    
    \item \textbf{Comprehensive Role-Based Framework}: Detailed specification of professional kitchen roles with appropriate authority boundaries, knowledge domains, and interaction patterns that reflect real-world organizational structures.
    
    \item \textbf{Extended Coherence Assessment}: Systematic evaluation across hundreds of related tasks to identify coherence degradation patterns and sustained performance capabilities over extended interaction sequences.
    
    \item \textbf{Multi-Dimensional Evaluation Metrics}: Sophisticated scoring rubrics that assess completeness, accuracy, efficiency, role adherence, safety compliance, and coordination awareness, providing nuanced insights beyond binary correctness measures.
    
    \item \textbf{Realistic Professional Context}: Grounding evaluation in authentic domain expertise requirements and procedural constraints that reflect the complexity of real-world coordination challenges.
\end{itemize}

Our implementation provides a scalable platform for evaluating multiple LLM architectures across diverse coordination scenarios, with applications extending beyond culinary environments to healthcare coordination, emergency response, business process management, and other domains requiring sustained multi-agent collaboration.

\subsection{Future Directions}

Several avenues for future research emerge from this work:

\begin{itemize}
    \item \textbf{Cross-Domain Validation}: Adaptation of the task-assignment evaluation paradigm to other professional domains such as healthcare teams, engineering projects, and emergency response scenarios to validate generalizability of findings.
    
    \item \textbf{Dynamic Interaction Assessment}: Extension beyond individual response evaluation to scenarios requiring real-time communication and negotiation between multiple LLM agents with potentially conflicting objectives.
    
    \item \textbf{Intervention Strategy Development}: Investigation of techniques for maintaining coherence over extended interactions, including external memory systems, periodic consistency checking, and hierarchical state summarization.
    
    \item \textbf{Hybrid Evaluation Approaches}: Integration of response-based assessment with simulated execution environments to bridge the gap between planning quality and operational effectiveness.
    
    \item \textbf{Specialization vs. Generalization Analysis}: Systematic comparison of role-specific fine-tuned models against general-purpose models to understand the trade-offs between specialization and flexibility in multi-agent coordination scenarios.
\end{itemize}

As LLMs become increasingly central to automated decision-making and coordination systems, benchmarks like MasterChef-Bench provide essential infrastructure for understanding their capabilities, limitations, and appropriate deployment contexts. The insights generated through systematic evaluation of task planning, role consistency, and coordination effectiveness will inform the development of more robust and reliable multi-agent AI systems for complex real-world applications.

The professional kitchen environment, with its combination of technical expertise requirements, hierarchical coordination needs, and high-stakes decision-making under time pressure, serves as an ideal microcosm for the challenges facing AI systems in professional contexts. Through MasterChef-Bench, we aim to contribute both to the fundamental understanding of LLM coordination capabilities and to the practical development of AI systems capable of sustained, coherent collaboration in complex organizational environments.

\section*{Acknowledgments}
We thank the culinary professionals who provided domain expertise for scenario validation, and the open-source community for the foundational technologies that enabled this work. Special appreciation to the maintainers of Python Fire, SQLAlchemy, and HuggingFace Transformers for their excellent libraries.

\bibliographystyle{unsrt}
\begin{thebibliography}{25}

\bibitem{backlund2025vending}
Backlund, A., \& Petersson, L. (2025).
\newblock Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents.
\newblock \emph{arXiv preprint arXiv:2502.15840}.

\bibitem{rakotonirina2025teammates}
Rakotonirina, N. C., Hamdy, M., Campos, J. A., Weber, L., Testoni, A., Fadaee, M., Pezzelle, S., \& Del Tredici, M. (2025).
\newblock From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions.
\newblock In \emph{Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics}, ACL 2025.
\newblock \emph{arXiv preprint arXiv:2502.13791}.

\bibitem{clark2021narratives}
Clark, E., Ross, A. S., Tan, C., Ji, Y., \& Smith, N. A. (2021).
\newblock Creative writing with a machine in the loop: Case studies on slogans and stories.
\newblock In \emph{Proceedings of the 26th International Conference on Intelligent User Interfaces}, pages 329--340.

\bibitem{dinan2020consistency}
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., \& Weston, J. (2020).
\newblock Wizard of wikipedia: Knowledge-powered conversational agents.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem{rashkin2021measuring}
Rashkin, H., Celikyilmaz, A., Wang, Y., \& Gao, J. (2021).
\newblock Measuring and improving consistency in pretrained language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9, 1012--1031.

\bibitem{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... \& Kiela, D. (2020).
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 33, 9459--9474.

\bibitem{borgeaud2022improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., ... \& Sifre, L. (2022).
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International Conference on Machine Learning}, pages 2206--2240.

\bibitem{park2023generative}
Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., \& Bernstein, M. S. (2023).
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In \emph{Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology}, pages 1--22.

\bibitem{chen2024coordinating}
Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., ... \& Li, Y. (2024).
\newblock Coordinating multi-agent planning through distributed reinforcement learning.
\newblock \emph{Nature Machine Intelligence}, 6(2), 142--157.

\bibitem{zhang2024hierarchy}
Zhang, L., Wang, M., Chen, S., \& Liu, Y. (2024).
\newblock Hierarchical coordination in multi-agent language model systems.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, pages 1456--1464.

\bibitem{valmeekam2023planning}
Valmeekam, K., Olmo, A., Sreedharan, S., \& Kambhampati, S. (2023).
\newblock Large language models still can't plan (a benchmark for llms on planning and reasoning about change).
\newblock In \emph{Advances in Neural Information Processing Systems}, 36, 48506--48524.

\bibitem{silver2024planning}
Silver, T., Dan, S., Srinivas, K., Tenenbaum, J. B., Kaelbling, L. P., \& Lozano-PÃ©rez, T. (2024).
\newblock Generalized planning in PDDL domains with pretrained large language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 38(18), 20256--20264.

\bibitem{shridhar2020alfred}
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., ... \& Fox, D. (2020).
\newblock Alfred: A benchmark for interpreting grounded instructions for everyday tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10740--10749.

\bibitem{puig2018virtualhome}
Puig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., \& Torralba, A. (2018).
\newblock Virtualhome: Simulating household activities via programs.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 8494--8502.

\bibitem{hendrycks2020measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., \& Steinhardt, J. (2020).
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem{srivastava2022beyond}
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., ... \& Wu, T. (2022).
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}.

\bibitem{kolve2017ai2}
Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., ... \& Farhadi, A. (2017).
\newblock AI2-THOR: An interactive 3D environment for visual AI.
\newblock \emph{arXiv preprint arXiv:1712.05474}.

\bibitem{savva2019habitat}
Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., ... \& Batra, D. (2019).
\newblock Habitat: A platform for embodied AI research.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 9339--9347.

\bibitem{wang2023voyager}
Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., ... \& Anandkumar, A. (2023).
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}.

\bibitem{liu2023agentbench}
Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., ... \& Zhang, Y. (2023).
\newblock AgentBench: Evaluating LLMs as agents.
\newblock \emph{arXiv preprint arXiv:2308.03688}.

\bibitem{xi2023rise}
Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., ... \& Gui, T. (2023).
\newblock The rise and potential of large language model based agents: A survey.
\newblock \emph{arXiv preprint arXiv:2309.07864}.

\bibitem{google2017fire}
Google. (2017).
\newblock Python Fire: A library for automatically generating command line interfaces.
\newblock \url{https://github.com/google/python-fire}.

\bibitem{wolf2020transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... \& Rush, A. M. (2020).
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45.

\bibitem{bayer2020sqlalchemy}
Bayer, M. (2020).
\newblock SQLAlchemy: The Database Toolkit for Python.
\newblock \url{https://www.sqlalchemy.org/}.

\bibitem{mckinney2010pandas}
McKinney, W. (2010).
\newblock Data structures for statistical computing in Python.
\newblock In \emph{Proceedings of the 9th Python in Science Conference}, pages 56--61.

\end{thebibliography}

\end{document}